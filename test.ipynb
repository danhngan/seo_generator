{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "Kiem tra tinh kha thi cua du an:\n",
    "- Kha nang tao bai chuan SEO\n",
    "- Kha nang trich xuat thong tin y hoc theo de muc\n",
    "- Kha nang theo cac trich xuat viet bai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of craigslist born?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI embedding\n",
    "\n",
    "Store cac muc nguyen nhan, dieu tri,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = Credentials.from_service_account_file(r'D:\\workspace\\important\\GCP\\genai.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project='ai-test-374607',\n",
    "            location='asia-southeast1',\n",
    "            credentials=cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class U:\n",
    "    u = None\n",
    "    b = 'b'\n",
    "\n",
    "U.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngantd\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "from vertexai.preview.language_models import TextEmbeddingModel as BatchTextEmbeddingModel\n",
    "def text_embedding(text) -> list:\n",
    "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "    text = TextEmbeddingInput(text=text, task_type='RETRIEVAL_DOCUMENT', title='general knowledge')\n",
    "    embeddings = model.get_embeddings([text])\n",
    "    # for embedding in embeddings:\n",
    "    #     vector = embedding.values\n",
    "        # print(f\"Length of Embedding Vector: {len(vector)}\")\n",
    "    # return vector\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite-Retrieve-Read Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Provide a better search query for \\\n",
    "web search engine to answer the given question, end \\\n",
    "the queries with ’**’. Question: \\\n",
    "{x} Answer:\"\"\"\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_prompt = hub.pull(\"langchain-ai/rewrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse(text):\n",
    "    return text.strip(\"**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a better search query for web search engine to answer the given question, end the queries with ’**’.  Question {x} Answer:\n"
     ]
    }
   ],
   "source": [
    "print(rewrite_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter = rewrite_prompt | ChatOpenAI(temperature=0) | StrOutputParser() | _parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "distracted_query = \"man that sam bankman fried trial was crazy! what is langchain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is Sam Bankman-Fried and what is Langchain?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewriter.invoke({\"x\": distracted_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo\n"
     ]
    }
   ],
   "source": [
    "class Meta(type):\n",
    "    def __init__(cls, name, bases, dct):\n",
    "        cls.attr = 100\n",
    "        cls.__init__ = Meta.child_fn\n",
    "        cls.__init__.__doc__ = Meta.child_fn.__doc__\n",
    "        print(name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def child_fn(self, *args):\n",
    "        \"\"\"init\"\"\"\n",
    "        self.args = args\n",
    "\n",
    "class Foo(metaclass=Meta):\n",
    "    pass\n",
    "\n",
    "\n",
    "x = Foo('a','b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'b')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vec Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngantd\\Anaconda3\\lib\\site-packages\\cryptography\\x509\\base.py:521: CryptographyDeprecationWarning: Parsed a negative serial number, which is disallowed by RFC 5280.\n",
      "  return rust_x509.load_der_x509_certificate(data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['ac-7ygojyv-shard-00-00.vgtf0pz.mongodb.net:27017', 'ac-7ygojyv-shard-00-02.vgtf0pz.mongodb.net:27017', 'ac-7ygojyv-shard-00-01.vgtf0pz.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Cluster0', authsource='admin', replicaset='atlas-n6oxzp-shard-0', tls=True, read_preference=Primary(), uuidrepresentation=3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# (db='mongo_vecdb',\n",
    "#         alias='mongo_vecdb',\n",
    "#         host='mongodb+srv://benhvienhongngoc:F8ekiVzss5OQexq5@cluster0.vgtf0pz.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0',\n",
    "#         authentication_source='admin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling multiple funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "@tool(\"disease-search-tool\", args_schema=SearchInput, return_direct=True)\n",
    "def gen_disease_search_query(query: str) -> str:\n",
    "    \"\"\"function that return the query string for diseases\"\"\"\n",
    "    return 'Diphtheria'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('disease-search-tool(query: str) -> str - function that return the query string for diseases',\n",
       " {'query': {'title': 'Query',\n",
       "   'description': 'should be a search query',\n",
       "   'type': 'string'}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_disease_search_query.description, gen_disease_search_query.args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hong ngoc post test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "posts = pd.read_csv('wp_posts.csv')\n",
    "print(posts.shape)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_posts = posts[(posts['post_type']=='post')&(pd.to_datetime(posts['post_date'])>pd.to_datetime('2021-01-01'))&posts[['post_title', 'post_content','guid']].notna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.storage import Blob\n",
    "from google.cloud import storage\n",
    "client = storage.Client(project='ai-test-374607',\n",
    "                            credentials=cred)\n",
    "def getStorageFileContent(blob:Blob, client:storage.Client):\n",
    "    \n",
    "    \n",
    "    file_content = blob.download_as_string(client=client).decode('utf-8')\n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_text_to_gcp_format(list_text):\n",
    "    res = []\n",
    "    for i,text in enumerate(list_text):\n",
    "        res.append(json.dumps({'content':text,'task_type': \"RETRIEVAL_DOCUMENT\",'id':str(i)}))\n",
    "    return '\\n'.join(res)\n",
    "\n",
    "def str_to_gcs(text:str, blob:Blob):\n",
    "    blob.upload_from_string(text)\n",
    "    print('uploaded')\n",
    "\n",
    "def multi_text_embedding(list_text, blob:Blob) -> list:\n",
    "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
    "    model = BatchTextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "    input_text = list_text_to_gcp_format(list_text=list_text)\n",
    "    str_to_gcs(input_text, blob)\n",
    "    batch_prediction_job=model.batch_predict(\n",
    "    dataset=['gs://'+'/'.join(blob.id.split('/')[:-1])],\n",
    "    destination_uri_prefix=\"gs://batch_embedding/output\")\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = Blob.from_string(uri=\"gs://batch_embedding/input/hn_posts.jsonl\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_text_embedding(content_posts['post_title'].to_list(), blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_prediction-model.jsonl','r') as f:\n",
    "    preds = f.read()\n",
    "preds = preds.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vecs = []\n",
    "for i, pred in enumerate(preds):\n",
    "    if len(pred)< 5:\n",
    "        continue\n",
    "    json_format = json.loads(pred)\n",
    "    vecs.append(\n",
    "        PointStruct(id=i+1, vector={'key':json_format['predictions'][0]['embeddings']['values'],\n",
    "                                  'content':[0]*768},\n",
    "                    payload={\"content\": json_format['instance']['content'][:1000]}))\n",
    "    if i%10 == 0:\n",
    "        operation_info = qd_client.upsert(\n",
    "    collection_name=\"post_collection\",\n",
    "    wait=True,\n",
    "    points=vecs,\n",
    ")\n",
    "        vecs = []\n",
    "\n",
    "        print(operation_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "serch_vec = text_embedding('bệnh gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoredPoint(id=1082, version=162, score=0.7701866, payload={'content': 'Sàng lọc bệnh lý Gan - Mật - Tụy tại Bệnh viện Hồng Ngọc'}, vector=None, shard_key=None), ScoredPoint(id=1544, version=162, score=0.7699643, payload={'content': '[THAM KHẢO] 14 điều được coi là “bổ gan” bạn nhất định phải biết'}, vector=None, shard_key=None), ScoredPoint(id=1364, version=162, score=0.7698804, payload={'content': 'U nguyên bào gan: Nguyên nhân, dấu hiệu và cách điều trị'}, vector=None, shard_key=None), ScoredPoint(id=1506, version=162, score=0.760731, payload={'content': 'Xơ gan còn bù: Nguyên nhân, biểu hiện và cách điều trị'}, vector=None, shard_key=None), ScoredPoint(id=1356, version=162, score=0.75999457, payload={'content': 'Tổng quan về bộ gen người'}, vector=None, shard_key=None)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_result = qd_client.search(\n",
    "    collection_name=\"post_collection\", query_vector=NamedVector(\n",
    "       name=\"key\",\n",
    "       vector=serch_vec[0].values), limit=5\n",
    ")\n",
    "\n",
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import uuid\n",
    "from functools import partial\n",
    "import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_seo.common.lang import DocumentLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_seo.crawler.api import Crawler, UpToDateCrawler\n",
    "from big_seo.llm.implement.index_mapper import SimpleQdrantHuggingFaceIndexMapperCreator,QdrantHuggingFaceIndexMapper\n",
    "from big_seo.llm.implement.indexer import WebPageIndexer, InPageIndexer, WebPageDocument,OutlineDocument\n",
    "from big_seo.llm.implement.parser import WebPageParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from big_seo.llm.implement.embedding_model import LLamaIndexEmbedding\n",
    "from big_seo.llm.implement.parser import OutlineDocumentCreator\n",
    "hug_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "embed_model = LLamaIndexEmbedding(hug_model)\n",
    "crawler = Crawler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_doc_creator = OutlineDocumentCreator()\n",
    "parser = WebPageParser(doc_creator=outline_doc_creator)\n",
    "parser_h2 = WebPageParser(doc_creator=outline_doc_creator)\n",
    "\n",
    "model = ChatOpenAI(name='gpt-4-turbo-preview',temperature=0.5)\n",
    "model_gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\",temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_topic = 'viêm tinh hoàn'\n",
    "webpages = crawler.search(main_topic,n=2)\n",
    "outlines = []\n",
    "for page in webpages:\n",
    "    webpage_doc = WebPageDocument(page)\n",
    "    parser.feed(webpage_doc)\n",
    "    outlines.append(parser.parse(in_doc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlines_headers = ['\\n'.join(map(lambda x: x[0].replace('\\n',''),o)) for o in outlines if len(o) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_outline_prompt = \"\"\"Bạn là một bác sĩ đa khoa, bạn có khả năng tạo những dàn ý bài viết đầy đủ, dễ hiểu cho các chủ đề liên quan y học. Khi tạo dàn ý, bạn cần dựa trên các dàn ý mẫu, các dàn ý mẫu được viết theo dạng <html tag> - <nội dung> , các dàn ý mẫu được phân tách nhau bằng ----, trong các dàn ý mẫu có thể có những mục không liên quan trực tiếp đến bài như 'Bài viết liên quan' 'Liên hệ' hãy bỏ qua các mục này. Nếu các dàn ý mẫu không chứa nội dung liên quan đến chủ đề bài viết, trả lời 'Tôi không biết'. Trong trường hợp các dàn ý mẫu có đề cập đến mục gây chú ý mạnh cho bệnh nhân khiến bệnh nhân muốn đến bệnh viện như 'sự nguy hiểm của bệnh' 'khi nào cần đến bệnh viện' thì hãy thêm vào dàn ý\n",
    "FORMAT: Dàn ý đầu ra cần tuân thủ chặt chẽ theo format. Mỗi ý một dòng. Không trả lời những thông tin không liên quan đến dàn ý.\n",
    "\n",
    "DÀN Ý MẪU: {example}\n",
    "\n",
    "Hãy nêu dàn ý cho bài viết về {topic}\n",
    "Trả lời:\"\"\"\n",
    "# print(gen_outline_prompt.format(example='\\n\\n----\\n\\n'.join(outlines_headers),topic=main_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res = model.invoke(gen_outline_prompt.format(example='\\n----\\n'.join(outlines_headers),topic=main_topic))\n",
    "model_gemini_res = model.invoke(gen_outline_prompt.format(example='\\n----\\n'.join(outlines_headers),topic=main_topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2 - Viêm tinh hoàn là gì?\n",
      "h2 - Nguyên nhân gây bệnh viêm tinh hoàn\n",
      "h3 - 1. Bệnh quai bị & các bệnh tự miễn\n",
      "h3 - 2. Bệnh lây qua đường tình dục STDs & hoạt động tình dục không an toàn\n",
      "h3 - 3. Nhiễm trùng đường tiết niệu\n",
      "h2 - Các triệu chứng và dấu hiệu của bệnh viêm tinh hoàn\n",
      "h2 - Nguy cơ mắc bệnh viêm tinh hoàn\n",
      "h3 - 1. Đối tượng\n",
      "h3 - 2. Yếu tố rủi ro tăng nguy cơ mắc bệnh\n",
      "h2 - Bệnh viêm tinh hoàn có nguy hiểm không?\n",
      "h2 - Khi nào cần gặp bác sĩ?\n",
      "h2 - Chẩn đoán bệnh viêm tinh hoàn\n",
      "h2 - Phương pháp điều trị viêm tinh hoàn\n",
      "h2 - Biện pháp phòng ngừa viêm tinh hoàn\n"
     ]
    }
   ],
   "source": [
    "# print(model_res.content)\n",
    "org_gen_outline = model_res.content\n",
    "if len(model_gemini_res.content) < 1000:\n",
    "    print(model_gemini_res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2 - Viêm tinh hoàn là gì?\n",
      "h2 - Nguyên nhân gây viêm tinh hoàn\n",
      "h3 - 1. Bệnh quai bị và các bệnh tự miễn\n",
      "h3 - 2. Bệnh lây qua đường tình dục STDs và hoạt động tình dục không an toàn\n",
      "h3 - 3. Nhiễm trùng đường tiết niệu\n",
      "h2 - Triệu chứng và dấu hiệu của viêm tinh hoàn\n",
      "h2 - Nguy cơ mắc bệnh viêm tinh hoàn\n",
      "h3 - 1. Đối tượng mắc bệnh\n",
      "h3 - 2. Yếu tố rủi ro tăng nguy cơ mắc bệnh\n",
      "h2 - Sự nguy hiểm của viêm tinh hoàn\n",
      "h2 - Khi nào cần đến bệnh viện khi mắc viêm tinh hoàn\n",
      "h2 - Chẩn đoán và điều trị viêm tinh hoàn\n",
      "h2 - Biện pháp phòng ngừa viêm tinh hoàn\n"
     ]
    }
   ],
   "source": [
    "if len(org_gen_outline) < 1000:\n",
    "    print(org_gen_outline)\n",
    "else:\n",
    "    input('continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_is_heading(text:str):\n",
    "    \"\"\"check if text is following heading format\"\"\"\n",
    "    if len(text) > 2 and text.startswith('h') and text[1].isdigit():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlines_to_h2(outline):\n",
    "    res = []\n",
    "    i = 0\n",
    "    while i<len(outline):\n",
    "        if check_is_heading(outline[i]):\n",
    "            res.append((outline[i],[]))\n",
    "            i+=1\n",
    "            while i<len(outline) and check_is_heading(outline[i]) and outline[i][:2] > 'h2':\n",
    "                res[-1][1].append(outline[i])\n",
    "                i+=1\n",
    "        else:\n",
    "            i += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_outline = list(map(lambda x:x.strip(),model_res.content.split('\\n')))\n",
    "h2_gen_outline = outlines_to_h2(gen_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prompt = \"\"\"Bạn là một bác sĩ giàu kinh nghiệm trên lĩnh vực {topic}, dựa trên KIẾN THỨC được cung cấp bạn có khả năng trình bày lại theo hướng cung cấp kiến thức chuyên sâu, rõ ràng\n",
    "\n",
    "\n",
    "BẮT BUỘC: nếu trong phần KIẾN THỨC không chứa nội dung liên quan đến mục bạn đang viết hãy trả lời 'Tôi không biết'\n",
    "\n",
    "\n",
    "########\n",
    "KIẾN THỨC: {knowledge}\n",
    "\n",
    "########\n",
    "NHIỆM VỤ: hãy viết một đoạn từ 300 đến 500 từ về '{headline}' {sub_headline_instruction} Bạn chỉ viết một mục trong bài viết lớn hơn, nên chỉ viết các thông tin liên quan đến '{headline}' không viết các thông tin khác\n",
    "\n",
    "TRẢ LỜI:\"\"\"\n",
    "\n",
    "h1_prompt = \"\"\"Bạn là một bác sĩ giàu kinh nghiệm trong lĩnh vực {topic}, dựa vào KIẾN THỨC được cung cấp bạn có thể trình bày lại ngắn gọn dễ hiểu\n",
    "\n",
    "BẮT BUỘC: nếu trong phần KIẾN THỨC không chứa nội dung liên quan đến mục bạn đang viết hãy trả lời 'Tôi không biết'\n",
    "\n",
    "########\n",
    "KIẾN THỨC: {knowledge}\n",
    "\n",
    "NHIỆM VỤ: Hãy viết một đoạn giới thiệu ngắn từ 200 đến 300 từ về chủ đề {topic} trong nêu các phần cụ thể như nguyên nhân, cách điều trị\n",
    "\n",
    "TRẢ LỜI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = SimpleQdrantHuggingFaceIndexMapperCreator(in_doc=True)\n",
    "parser_h2.parse = partial(parser_h2.parse, nested=False, in_doc=True,concat_to='h2')\n",
    "webpage_indexer = InPageIndexer(embedding_model=embed_model,\n",
    "                                parser=parser_h2,\n",
    "                                index_mapper_creator=creator)\n",
    "for page in webpages:\n",
    "    page_doc = WebPageDocument(page)\n",
    "    webpage_indexer.invoke(page_doc)\n",
    "\n",
    "webpage_index_mapper = webpage_indexer.get_index_mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_seo.llm.implement.retriever import DirectRetriever\n",
    "from big_seo.llm.core.common import IPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(IPrompt):\n",
    "    def __init__(self, prompt) -> None:\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        return self.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DirectRetriever(embedding_model=embed_model,\n",
    "                            indexer=webpage_indexer,\n",
    "                            index_mapper_creator=creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HashableOutlineDoc(OutlineDocument):\n",
    "#     def __hash__(self):\n",
    "#         return hash(self.doc_id)\n",
    "    \n",
    "#     def __eq__(self, __value: object) -> bool:\n",
    "#         return hash(self.doc_id) == hash(__value)\n",
    "\n",
    "# def get_unique_retrieved_docs(retrieved_docs):\n",
    "#     docs = map(lambda x: x.get_doc(),retrieved_docs)\n",
    "#     return set([HashableOutlineDoc(header=doc.header, content=doc.content, doc_id=doc.doc_id) for doc in docs])\n",
    "\n",
    "# def get_gen_knowledge_fn(embed_model,creator,webpage_index_mapper):\n",
    "#     def sub(outline):\n",
    "#         prompts = [outline[0]]\n",
    "#         prompts.extend(outline[1])\n",
    "#         docs = set()\n",
    "#         for prompt in prompts:\n",
    "#             vec = embed_model.get_embedding(prompt)\n",
    "#             db_vec = creator.get_db_familiar_vector(vec=vec, get=True)\n",
    "#             retrieved_docs = webpage_index_mapper.search_doc(vec=db_vec)\n",
    "#             docs = docs.union(get_unique_retrieved_docs(retrieved_docs[:7]))\n",
    "#             # print(len(docs))\n",
    "#         return '\\n----\\n'.join(map(lambda x: x.header +'\\n'+ x.get_doc_content(),list(docs)[:5]))\n",
    "#     return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_knowledge_fn(embed_model,creator,webpage_index_mapper):\n",
    "    def sub(outline):\n",
    "        prompts = [outline[0]]\n",
    "        prompts.extend(outline[1])\n",
    "        docs = set()\n",
    "        for prompt in prompts:\n",
    "            retrieved_docs = retriever.invoke(Prompt(prompt=prompt),limit=5)\n",
    "            docs = docs.union(retrieved_docs)\n",
    "            # print(len(docs))\n",
    "        return '\\n----\\n'.join(map(lambda x: x.get_doc_meta()['title'] +'\\n'+ x.get_doc().get_doc_content(),list(docs)[:5]))\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_knowledge = get_gen_knowledge_fn(embed_model,creator,webpage_index_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gen_content = []\n",
    "error_knowledge = []\n",
    "outline_with_knowledge = []\n",
    "# for o in h2_gen_outline:\n",
    "for o in [(o,[]) for o in org_gen_outline.split('\\n')]:\n",
    "    full_gen_content.append(o[0])\n",
    "    if o[0].startswith('h1'):\n",
    "        knowledge = get_knowledge(outline=o)\n",
    "        prompt = h1_prompt.format(topic=main_topic, knowledge=knowledge)\n",
    "    else:\n",
    "        knowledge = get_knowledge(outline=o)\n",
    "        if len(o[1]) > 0:\n",
    "            sub_headline ='Trong đoạn cần đề cập đến các đề mục '+' '.join(o[1])\n",
    "        else:\n",
    "            sub_headline = ''\n",
    "        \n",
    "        prompt = partial_prompt.format(topic=main_topic, knowledge=knowledge,\n",
    "                               headline = o[0],\n",
    "                               sub_headline_instruction=sub_headline)\n",
    "    \n",
    "    res = model.invoke(prompt)\n",
    "    outline_with_knowledge.append((o[0],res.content,knowledge))\n",
    "    if 'tôi không biết' in res.content.lower():\n",
    "        print(o[0])\n",
    "        error_knowledge.append((o[0], knowledge))\n",
    "    full_gen_content.append(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'test_output/{main_topic}_raw.txt','w') as f:\n",
    "    f.write('\\n'.join(full_gen_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt=\"\"\"Bạn là một bác sĩ giàu kinh nghiệm trong lĩnh vực {topic}\n",
    "Dưới đây là một mẫu bài viết về {topic},\n",
    "\n",
    "Bài mẫu: {example}\n",
    "\n",
    "\n",
    "YÊU CẦU bạn hãy viết lại bài viết này thành một bài viết chuyên sâu khoảng 1600 từ theo hướng tối ưu chuẩn SEO, đảm bảo mật độ từ khóa chính {topic} tốt.\n",
    "Với mỗi ý hãy nêu kèm giải thích chọn cách trình bày như liệt kê từng dòng, so sánh hợp lý\n",
    "\n",
    "BẮT BUỘC: bạn không cung cấp bất cứ kiến thức nào khác ngoài bài mẫu\n",
    "\n",
    "TRẢ LỜI\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = model_gemini.invoke(final_prompt.format(topic=main_topic,\n",
    "                                             outline=org_gen_outline,\n",
    "                                             example = '\\n'.join(full_gen_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'test_output/gemini_rewrite_{main_topic}.txt','w') as f:\n",
    "    f.write(final_res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = model.invoke(final_prompt.format(topic=main_topic,\n",
    "                                             outline=org_gen_outline,\n",
    "                                             example = '\\n'.join(full_gen_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'test_output/gpt_rewrite_{main_topic}.txt','w') as f:\n",
    "    f.write(final_res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('template.txt','r') as f:\n",
    "    template = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.append(template.format(_id = 0,\n",
    "                           cur_state='cur',\n",
    "                           outline=org_gen_outline.replace('\\n','<br>'),\n",
    "                           model_output='<br>'.join(full_gen_content).replace('\\n','<br>'),\n",
    "                           knowledge=''))\n",
    "for i,(outline, model_output, knowledge) in enumerate(outline_with_knowledge):\n",
    "    _id = i + 1\n",
    "    cur_state = 'hidden'\n",
    "    temp = template.format(_id = _id,\n",
    "                           cur_state=cur_state,\n",
    "                           outline=outline,\n",
    "                           model_output=model_output,\n",
    "                           knowledge=knowledge.replace('\\n','<br>'))\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.txt','r') as f:\n",
    "    index_page = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_page = index_page.format(data='\\n'.join(data))\n",
    "with open('index.html','w') as f:\n",
    "    f.write(index_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.683333333333334"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "# 10:13 -> 10:19\n",
    "(end - start).seconds/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test db model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractclassmethod, abstractstaticmethod,abstractmethod\n",
    "\n",
    "# class BaseVectorStoreModel:\n",
    "#     _subclasses: list = []\n",
    "#     def __init_subclass__(cls):\n",
    "#         BaseVectorStoreModel._subclasses.append(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVectorStoreModel:\n",
    "\n",
    "    @abstractstaticmethod\n",
    "    def init_instance(vec_db):\n",
    "        pass\n",
    "\n",
    "    @abstractstaticmethod\n",
    "    def query(vec_db):\n",
    "        pass\n",
    "\n",
    "    @abstractstaticmethod\n",
    "    def close_instance(vec_db):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOne(BaseVectorStoreModel):\n",
    "    _collections: dict = {}\n",
    "    \n",
    "    def __init_subclass__(cls):\n",
    "        BaseOne._collections[cls.__collection__] = cls\n",
    "\n",
    "    @staticmethod\n",
    "    def init_instance(vec_db):\n",
    "        print('init')\n",
    "\n",
    "    @staticmethod\n",
    "    def query(vec_db):\n",
    "        print('query')\n",
    "        \n",
    "    @staticmethod\n",
    "    def close_instance(vec_db):\n",
    "        print('close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7e396061-4171-4a9d-9ac3-3a117e63a782'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddingDocumentModel(BaseOne):\n",
    "    __collection__: str = 'None'\n",
    "    _id: bytes\n",
    "    head: str\n",
    "    doc: str\n",
    "    vec: dict[str, list]\n",
    "\n",
    "    meta: dict = None\n",
    "\n",
    "    def _to_db(self):\n",
    "        print('dump to db')\n",
    "\n",
    "    def _to_doc(self):\n",
    "        print('retrieved docs from db')\n",
    "\n",
    "\n",
    "class BaseEmbeddingDocumentModelTwo(BaseOne):\n",
    "    __collection__: str = 'None'\n",
    "    _id: bytes\n",
    "    head: str\n",
    "    doc: str\n",
    "    vec: dict[str, list]\n",
    "\n",
    "    meta: dict = None\n",
    "\n",
    "    def _to_db(self):\n",
    "        print('dump to db')\n",
    "\n",
    "    def _to_doc(self):\n",
    "        print('retrieved docs from db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddingDocumentModelThree(Base_one):\n",
    "    __collection__: str = 'None'\n",
    "    _id: bytes\n",
    "    head: str\n",
    "    doc: str\n",
    "    vec: dict[str, list]\n",
    "\n",
    "    meta: dict = None\n",
    "\n",
    "    def _to_db(self):\n",
    "        print('dump to db')\n",
    "\n",
    "    def _to_doc(self):\n",
    "        print('retrieved docs from db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'None': __main__.BaseEmbeddingDocumentModelTwo}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BaseOne._collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myt = '''230545043\n",
    "230528327\n",
    "230538089\n",
    "171688072\n",
    "240052385\n",
    "230395363\n",
    "240058580\n",
    "240011810\n",
    "240025907\n",
    "240024581\n",
    "230516444\n",
    "230518280\n",
    "230522009\n",
    "230508908\n",
    "230520581\n",
    "230502086\n",
    "230483147\n",
    "230546378\n",
    "240005057\n",
    "230547092\n",
    "230537531\n",
    "230537828\n",
    "230537816\n",
    "230529542\n",
    "230534006\n",
    "230537696\n",
    "230542862\n",
    "230544287\n",
    "240050444\n",
    "240050756\n",
    "240046382\n",
    "240046928\n",
    "240012068\n",
    "240025688\n",
    "171030949\n",
    "230517830\n",
    "230028306\n",
    "230020022\n",
    "240005984\n",
    "171432187\n",
    "230466935\n",
    "12036413\n",
    "230545049\n",
    "230538635\n",
    "171260211\n",
    "240038570\n",
    "240052694\n",
    "240038564\n",
    "240042173\n",
    "240047390\n",
    "240050393\n",
    "240011858\n",
    "240026504\n",
    "240035237\n",
    "240025610\n",
    "240035663\n",
    "230520608\n",
    "13064155\n",
    "230524538\n",
    "230514767\n",
    "230521775\n",
    "230524862\n",
    "230501339\n",
    "14030799\n",
    "240001664\n",
    "240011084\n",
    "240011363\n",
    "230547515\n",
    "240001655\n",
    "172980284\n",
    "240011567\n",
    "230539232\n",
    "230542967\n",
    "230544008\n",
    "230527862\n",
    "230528402\n",
    "230528468\n",
    "230545325\n",
    "17047023\n",
    "240038567\n",
    "240043049\n",
    "240043502\n",
    "240049256\n",
    "240023054\n",
    "230486117\n",
    "230494226\n",
    "171066857\n",
    "230548637\n",
    "15041581\n",
    "240004835\n",
    "230287118\n",
    "230537396\n",
    "230542811\n",
    "230542190\n",
    "171842568\n",
    "240046406\n",
    "240043091\n",
    "240050117\n",
    "240053822\n",
    "240012179\n",
    "230238844\n",
    "171809556\n",
    "240032720\n",
    "240036287\n",
    "13023676\n",
    "230522348\n",
    "171833678\n",
    "240001268\n",
    "240003683\n",
    "240008054\n",
    "171103335\n",
    "240008987\n",
    "240009998\n",
    "ma_y_te\n",
    "230544008\n",
    "240035345\n",
    "17047023\n",
    "230287118\n",
    "240038567\n",
    "230545325\n",
    "240043049\n",
    "240043502\n",
    "240049256\n",
    "15041581\n",
    "240006290\n",
    "230539232\n",
    "240005576\n",
    "240023054\n",
    "230542967\n",
    "230527862\n",
    "171066857\n",
    "230528402\n",
    "230528468\n",
    "230548637\n",
    "240004835\n",
    "230486117\n",
    "11043705\n",
    "230472359\n",
    "230448307\n",
    "171688072\n",
    "240047681\n",
    "230547092\n",
    "240041066\n",
    "240052385\n",
    "230395363\n",
    "240058580\n",
    "230546378\n",
    "230538089\n",
    "230516444\n",
    "172739094\n",
    "230545043\n",
    "230528327\n",
    "240005057\n",
    "230516288\n",
    "230518280\n",
    "230508908\n",
    "230520581\n",
    "230458684\n",
    "230502086\n",
    "230460132\n",
    "230483147\n",
    "240032720\n",
    "240036287\n",
    "230238844\n",
    "240042065\n",
    "171809556\n",
    "240003683\n",
    "240046406\n",
    "240054434\n",
    "240012179\n",
    "171842568\n",
    "171103335\n",
    "240029642\n",
    "240043091\n",
    "240050117\n",
    "171431526\n",
    "240053822\n",
    "240008054\n",
    "240012731\n",
    "230315179\n",
    "240008987\n",
    "240009998\n",
    "230545376\n",
    "240032693\n",
    "230532113\n",
    "13023676\n",
    "171260206\n",
    "230537396\n",
    "230542811\n",
    "230536412\n",
    "230542190\n",
    "171833678\n",
    "240001268\n",
    "172902590\n",
    "230522348\n",
    "172796376\n",
    "240012068\n",
    "240025688\n",
    "240050444\n",
    "240050756\n",
    "240046382\n",
    "240046928\n",
    "171432187\n",
    "230537531\n",
    "171030949\n",
    "240034181\n",
    "230517830\n",
    "230536481\n",
    "230537816\n",
    "230537828\n",
    "15115537\n",
    "230529542\n",
    "230534006\n",
    "230537696\n",
    "230542862\n",
    "230544287\n",
    "240005984\n",
    "230028306\n",
    "230516042\n",
    "230473376\n",
    "230020022\n",
    "240038570\n",
    "14030799\n",
    "240035237\n",
    "240026504\n",
    "240051893\n",
    "240052694\n",
    "240038564\n",
    "230547515\n",
    "240035663\n",
    "240042173\n",
    "240047390\n",
    "240050393\n",
    "172980284\n",
    "230501339\n",
    "240011858\n",
    "240014099\n",
    "230466935\n",
    "240025610\n",
    "230538635\n",
    "13064155\n",
    "171469115\n",
    "12036413\n",
    "230545049\n",
    "171260211\n",
    "240001664\n",
    "240011084\n",
    "240011363\n",
    "240001655\n",
    "230175608\n",
    "230520608\n",
    "230524538\n",
    "230306663\n",
    "230455346\n",
    "230524862'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT UF_CRM_1644459004214 FROM b_crm_deal d LEFT JOIN b_uts_crm_contact c ON d.`CONTACT_ID` = c.`VALUE_ID` WHERE `CATEGORY_ID` = 7 AND `STAGE_SEMANTIC_ID` = 'S' AND `DATE_CREATE` > '2024-01-01' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
